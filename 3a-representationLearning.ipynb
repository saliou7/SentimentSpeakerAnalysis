{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP & representation learning: Neural Embeddings, Text Classification\n",
    "\n",
    "\n",
    "To use statistical classifiers with text, it is first necessary to vectorize the text. In the first practical session we explored the **Bag of Word (BoW)** model. \n",
    "\n",
    "Modern **state of the art** methods uses  embeddings to vectorize the text before classification in order to avoid feature engineering.\n",
    "\n",
    "## [Dataset](https://thome.isir.upmc.fr/classes/RITAL/json_pol.json)\n",
    "\n",
    "\n",
    "## \"Modern\" NLP pipeline\n",
    "\n",
    "By opposition to the **bag of word** model, in the modern NLP pipeline everything is **embeddings**. Instead of encoding a text as a **sparse vector** of length $D$ (size of feature dictionnary) the goal is to encode the text in a meaningful dense vector of a small size $|e| <<< |D|$. \n",
    "\n",
    "\n",
    "The raw classification pipeline is then the following:\n",
    "\n",
    "```\n",
    "raw text ---|embedding table|-->  vectors --|Neural Net|--> class \n",
    "```\n",
    "\n",
    "\n",
    "### Using a  language model:\n",
    "\n",
    "How to tokenize the text and extract a feature dictionnary is still a manual task. To directly have meaningful embeddings, it is common to use a pre-trained language model such as `word2vec` which we explore in this practical.\n",
    "\n",
    "In this setting, the pipeline becomes the following:\n",
    "```\n",
    "      \n",
    "raw text ---|(pre-trained) Language Model|--> vectors --|classifier (or fine-tuning)|--> class \n",
    "```\n",
    "\n",
    "\n",
    "- #### Classic word embeddings\n",
    "\n",
    " - [Word2Vec](https://arxiv.org/abs/1301.3781)\n",
    " - [Glove](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "\n",
    "- #### bleeding edge language models techniques (see next)\n",
    "\n",
    " - [UMLFIT](https://arxiv.org/abs/1801.06146)\n",
    " - [ELMO](https://arxiv.org/abs/1802.05365)\n",
    " - [GPT](https://blog.openai.com/language-unsupervised/)\n",
    " - [BERT](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Goal of this session:\n",
    "\n",
    "1. Train word embeddings on training dataset\n",
    "2. Tinker with the learnt embeddings and see learnt relations\n",
    "3. Tinker with pre-trained embeddings.\n",
    "4. Use those embeddings for classification\n",
    "5. Compare different embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 0: Loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews :  25000\n",
      "----> # of positive :  12500\n",
      "----> # of negative :  12500\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Loading json\n",
    "file = './datasets/json_pol.json'\n",
    "with open(file,encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "\n",
    "# Quick Check\n",
    "counter = Counter((x[1] for x in data))\n",
    "print(\"Number of reviews : \", len(data))\n",
    "print(\"----> # of positive : \", counter[1])\n",
    "print(\"----> # of negative : \", counter[0])\n",
    "print(\"\")\n",
    "print(data[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec: Quick Recap\n",
    "\n",
    "**[Word2Vec](https://arxiv.org/abs/1301.3781) is composed of two distinct language models (CBOW and SG), optimized to quickly learn word vectors**\n",
    "\n",
    "\n",
    "given a random text: `i'm taking the dog out for a walk`\n",
    "\n",
    "\n",
    "\n",
    "### (a) Continuous Bag of Word (CBOW)\n",
    "    -  predicts a word given a context\n",
    "    \n",
    "maximizing `p(dog | i'm taking the ___ out for a walk)`\n",
    "    \n",
    "### (b) Skip-Gram (SG)               \n",
    "    -  predicts a context given a word\n",
    "    \n",
    " maximizing `p(i'm taking the out for a walk | dog)`\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: train a language model (word2vec)\n",
    "\n",
    "Gensim has one of [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) fastest implementation.\n",
    "\n",
    "\n",
    "### Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gensim not installed yet\n",
    "# ! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 19:12:06,348 : INFO : collecting all words and their counts\n",
      "2024-02-29 19:12:06,348 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-02-29 19:12:06,849 : INFO : PROGRESS: at sentence #10000, processed 2301366 words, keeping 153853 word types\n",
      "2024-02-29 19:12:07,282 : INFO : PROGRESS: at sentence #20000, processed 4553558 words, keeping 240043 word types\n",
      "2024-02-29 19:12:07,502 : INFO : collected 276678 word types from a corpus of 5713167 raw words and 25000 sentences\n",
      "2024-02-29 19:12:07,502 : INFO : Creating a fresh vocabulary\n",
      "2024-02-29 19:12:07,684 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 48208 unique words (17.42% of original 276678, drops 228470)', 'datetime': '2024-02-29T19:12:07.684063', 'gensim': '4.3.2', 'python': '3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2024-02-29 19:12:07,684 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 5389596 word corpus (94.34% of original 5713167, drops 323571)', 'datetime': '2024-02-29T19:12:07.684063', 'gensim': '4.3.2', 'python': '3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2024-02-29 19:12:07,883 : INFO : deleting the raw counts dictionary of 276678 items\n",
      "2024-02-29 19:12:07,890 : INFO : sample=0.001 downsamples 44 most-common words\n",
      "2024-02-29 19:12:07,891 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 4165010.0598832574 word corpus (77.3%% of prior 5389596)', 'datetime': '2024-02-29T19:12:07.891349', 'gensim': '4.3.2', 'python': '3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2024-02-29 19:12:08,166 : INFO : estimated required memory for 48208 words and 100 dimensions: 62670400 bytes\n",
      "2024-02-29 19:12:08,166 : INFO : resetting layer weights\n",
      "2024-02-29 19:12:08,203 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-29T19:12:08.203569', 'gensim': '4.3.2', 'python': '3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'build_vocab'}\n",
      "2024-02-29 19:12:08,204 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 48208 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-02-29T19:12:08.204265', 'gensim': '4.3.2', 'python': '3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
      "2024-02-29 19:12:09,221 : INFO : EPOCH 0 - PROGRESS: at 11.60% examples, 476310 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:10,255 : INFO : EPOCH 0 - PROGRESS: at 23.09% examples, 469884 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:11,262 : INFO : EPOCH 0 - PROGRESS: at 34.74% examples, 474062 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-29 19:12:12,260 : INFO : EPOCH 0 - PROGRESS: at 46.46% examples, 475730 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-29 19:12:13,265 : INFO : EPOCH 0 - PROGRESS: at 58.02% examples, 474798 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:14,298 : INFO : EPOCH 0 - PROGRESS: at 69.43% examples, 474114 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:15,299 : INFO : EPOCH 0 - PROGRESS: at 80.81% examples, 472020 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:16,307 : INFO : EPOCH 0 - PROGRESS: at 92.17% examples, 473079 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-29 19:12:17,001 : INFO : EPOCH 0: training on 5713167 raw words (4165424 effective words) took 8.8s, 473644 effective words/s\n",
      "2024-02-29 19:12:18,025 : INFO : EPOCH 1 - PROGRESS: at 11.40% examples, 468287 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:19,057 : INFO : EPOCH 1 - PROGRESS: at 23.27% examples, 471795 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:20,062 : INFO : EPOCH 1 - PROGRESS: at 34.74% examples, 472193 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:21,076 : INFO : EPOCH 1 - PROGRESS: at 46.10% examples, 470469 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:22,115 : INFO : EPOCH 1 - PROGRESS: at 58.19% examples, 471503 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:23,120 : INFO : EPOCH 1 - PROGRESS: at 69.78% examples, 474540 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:24,116 : INFO : EPOCH 1 - PROGRESS: at 81.47% examples, 474439 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-29 19:12:25,122 : INFO : EPOCH 1 - PROGRESS: at 92.82% examples, 475290 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-29 19:12:25,732 : INFO : EPOCH 1: training on 5713167 raw words (4164440 effective words) took 8.7s, 476932 effective words/s\n",
      "2024-02-29 19:12:26,737 : INFO : EPOCH 2 - PROGRESS: at 10.84% examples, 455763 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:27,745 : INFO : EPOCH 2 - PROGRESS: at 22.58% examples, 467638 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-29 19:12:28,768 : INFO : EPOCH 2 - PROGRESS: at 33.91% examples, 465944 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:29,781 : INFO : EPOCH 2 - PROGRESS: at 45.56% examples, 469649 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-29 19:12:30,781 : INFO : EPOCH 2 - PROGRESS: at 56.76% examples, 466739 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-29 19:12:31,799 : INFO : EPOCH 2 - PROGRESS: at 68.36% examples, 469264 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:32,797 : INFO : EPOCH 2 - PROGRESS: at 80.49% examples, 472144 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:33,798 : INFO : EPOCH 2 - PROGRESS: at 91.73% examples, 472359 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-29 19:12:34,567 : INFO : EPOCH 2: training on 5713167 raw words (4164455 effective words) took 8.8s, 471606 effective words/s\n",
      "2024-02-29 19:12:35,581 : INFO : EPOCH 3 - PROGRESS: at 10.84% examples, 448950 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:36,623 : INFO : EPOCH 3 - PROGRESS: at 22.73% examples, 461673 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:37,628 : INFO : EPOCH 3 - PROGRESS: at 34.38% examples, 468964 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:38,630 : INFO : EPOCH 3 - PROGRESS: at 45.91% examples, 471186 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:39,637 : INFO : EPOCH 3 - PROGRESS: at 57.84% examples, 473182 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:40,640 : INFO : EPOCH 3 - PROGRESS: at 69.05% examples, 472665 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:41,678 : INFO : EPOCH 3 - PROGRESS: at 81.16% examples, 473560 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-29 19:12:42,695 : INFO : EPOCH 3 - PROGRESS: at 92.82% examples, 475287 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-29 19:12:43,314 : INFO : EPOCH 3: training on 5713167 raw words (4164901 effective words) took 8.7s, 476114 effective words/s\n",
      "2024-02-29 19:12:44,313 : INFO : EPOCH 4 - PROGRESS: at 11.21% examples, 470902 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:45,329 : INFO : EPOCH 4 - PROGRESS: at 22.76% examples, 471816 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:46,337 : INFO : EPOCH 4 - PROGRESS: at 34.59% examples, 477545 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:47,343 : INFO : EPOCH 4 - PROGRESS: at 46.27% examples, 478010 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:48,340 : INFO : EPOCH 4 - PROGRESS: at 58.19% examples, 479416 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:49,369 : INFO : EPOCH 4 - PROGRESS: at 69.08% examples, 474831 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:50,416 : INFO : EPOCH 4 - PROGRESS: at 81.47% examples, 476239 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:51,429 : INFO : EPOCH 4 - PROGRESS: at 92.97% examples, 476897 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:12:52,024 : INFO : EPOCH 4: training on 5713167 raw words (4164841 effective words) took 8.7s, 477883 effective words/s\n",
      "2024-02-29 19:12:52,024 : INFO : Word2Vec lifecycle event {'msg': 'training on 28565835 raw words (20824061 effective words) took 43.8s, 475078 effective words/s', 'datetime': '2024-02-29T19:12:52.024950', 'gensim': '4.3.2', 'python': '3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
      "2024-02-29 19:12:52,024 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=48208, vector_size=100, alpha=0.025>', 'datetime': '2024-02-29T19:12:52.024950', 'gensim': '4.3.2', 'python': '3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "text = [t.split() for t,p in data]\n",
    "\n",
    "\n",
    "# the following configuration is the default configuration\n",
    "w2v = gensim.models.word2vec.Word2Vec(sentences=text,\n",
    "                                vector_size=100, window=5,               ### here we train a cbow model \n",
    "                                min_count=5,                      \n",
    "                                sample=0.001, workers=3,\n",
    "                                sg=1, hs=0, negative=5,        ### set sg to 1 to train a sg model\n",
    "                                cbow_mean=1, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "533"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 13:36:19,380 : INFO : loading Word2Vec object from W2v-movies.dat\n",
      "2024-02-29 13:36:19,434 : INFO : loading wv recursively from W2v-movies.dat.wv.* with mmap=None\n",
      "2024-02-29 13:36:19,435 : INFO : setting ignored attribute cum_table to None\n",
      "2024-02-29 13:36:19,705 : INFO : Word2Vec lifecycle event {'fname': 'W2v-movies.dat', 'datetime': '2024-02-29T13:36:19.705738', 'gensim': '4.3.2', 'python': '3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "# Worth it to save the previous embedding\n",
    "#w2v.save(\"W2v-movies.dat\")\n",
    "# You will be able to reload them:\n",
    "w2v = gensim.models.Word2Vec.load(\"W2v-movies.dat\")\n",
    "# and you can continue the learning process if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Test learnt embeddings\n",
    "\n",
    "The word embedding space directly encodes similarities between words: the vector coding for the word \"great\" will be closer to the vector coding for \"good\" than to the one coding for \"bad\". Generally, [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) is the distance used when considering distance between vectors.\n",
    "\n",
    "KeyedVectors have a built in [similarity](https://radimrehurek.com/gensim/models /keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.similarity) method to compute the cosine similarity between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great and good: 0.7646114\n",
      "great and bad: 0.48655832\n",
      "king and man: 0.5934056\n"
     ]
    }
   ],
   "source": [
    "# is great really closer to good than to bad ?\n",
    "print(\"great and good:\",w2v.wv.similarity(\"great\",\"good\"))\n",
    "print(\"great and bad:\",w2v.wv.similarity(\"great\",\"bad\"))\n",
    "print(\"king and man:\",w2v.wv.similarity(\"king\",\"man\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since cosine distance encodes similarity, neighboring words are supposed to be similar. The [most_similar](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.most_similar) method returns the `topn` words given a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actor,', 0.8326389789581299),\n",
       " ('actor.', 0.7580764889717102),\n",
       " ('Reeves', 0.7533103227615356),\n",
       " ('actress', 0.7248632311820984),\n",
       " ('role,', 0.7208895087242126)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The query can be as simple as a word, such as \"movie\"\n",
    "\n",
    "# Try changing the word\n",
    "# w2v.wv.most_similar(\"movie\",topn=5) # 5 most similar words\n",
    "# w2v.wv.most_similar(\"awesome\",topn=5)\n",
    "w2v.wv.most_similar(\"actor\",topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it can be a more complicated query\n",
    "Word embedding spaces tend to encode much more.\n",
    "\n",
    "The most famous exemple is: `vec(king) - vec(man) + vec(woman) => vec(queen)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mexico', 0.6771059036254883),\n",
       " ('downtown', 0.6716404557228088),\n",
       " ('Angeles', 0.6657335162162781),\n",
       " ('London', 0.6620723009109497),\n",
       " ('California,', 0.6614657640457153),\n",
       " ('Mexico,', 0.6600666642189026)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is awesome - good + bad ?\n",
    "#w2v.wv.most_similar(positive=[\"awesome\",\"bad\"],negative=[\"good\"],topn=3)  \n",
    "\n",
    "#w2v.wv.most_similar(positive=[\"actor\",\"woman\"],negative=[\"man\"],topn=3) # do the famous exemple works for actor ?\n",
    "\n",
    "w2v.wv.most_similar(positive=[\"Paris\",\"France\"],negative=[\"English\"],topn=6) \n",
    "\n",
    "# Try other things like plurals for exemple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To test learnt \"synctactic\" and \"semantic\" similarities, Mikolov et al. introduced a special dataset containing a wide variety of three way similarities.**\n",
    "\n",
    "**You can download the dataset [here](https://thome.isir.upmc.fr/classes/RITAL/questions-words.txt).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 19:13:15,811 : INFO : Evaluating word analogies for top 300000 words in the model on ressources/questions-words.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 19:13:15,929 : INFO : capital-common-countries: 6.7% (6/90)\n",
      "2024-02-29 19:13:16,009 : INFO : capital-world: 2.8% (2/71)\n",
      "2024-02-29 19:13:16,044 : INFO : currency: 0.0% (0/28)\n",
      "2024-02-29 19:13:16,393 : INFO : city-in-state: 0.0% (0/329)\n",
      "2024-02-29 19:13:16,759 : INFO : family: 34.5% (118/342)\n",
      "2024-02-29 19:13:17,711 : INFO : gram1-adjective-to-adverb: 1.3% (12/930)\n",
      "2024-02-29 19:13:18,265 : INFO : gram2-opposite: 2.9% (16/552)\n",
      "2024-02-29 19:13:19,435 : INFO : gram3-comparative: 20.2% (254/1260)\n",
      "2024-02-29 19:13:20,060 : INFO : gram4-superlative: 7.3% (51/702)\n",
      "2024-02-29 19:13:20,722 : INFO : gram5-present-participle: 18.8% (142/756)\n",
      "2024-02-29 19:13:21,395 : INFO : gram6-nationality-adjective: 2.8% (22/792)\n",
      "2024-02-29 19:13:22,350 : INFO : gram7-past-tense: 16.7% (211/1260)\n",
      "2024-02-29 19:13:22,915 : INFO : gram8-plural: 4.2% (34/812)\n",
      "2024-02-29 19:13:23,516 : INFO : gram9-plural-verbs: 24.9% (188/756)\n",
      "2024-02-29 19:13:23,516 : INFO : Quadruplets with out-of-vocabulary words: 55.6%\n",
      "2024-02-29 19:13:23,516 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use \"dummy4unknown=True\"\n",
      "2024-02-29 19:13:23,516 : INFO : Total accuracy: 12.2% (1056/8680)\n"
     ]
    }
   ],
   "source": [
    "out = w2v.wv.evaluate_word_analogies(\"ressources/questions-words.txt\",case_insensitive=True)  #original semantic syntactic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When training the w2v models on the review dataset, since it hasn't been learnt with a lot of data, it does not perform very well.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Loading a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Gensim, embeddings are loaded and can be used via the [\"KeyedVectors\"](https://radimrehurek.com/gensim/models/keyedvectors.html) class\n",
    "\n",
    "> Since trained word vectors are independent from the way they were trained (Word2Vec, FastText, WordRank, VarEmbed etc), they can be represented by a standalone structure, as implemented in this module.\n",
    "\n",
    ">The structure is called “KeyedVectors” and is essentially a mapping between entities and vectors. Each entity is identified by its string id, so this is a mapping between {str => 1D numpy array}.\n",
    "\n",
    ">The entity typically corresponds to a word (so the mapping maps words to 1D vectors), but for some models, they key can also correspond to a document, a graph node etc. To generalize over different use-cases, this module calls the keys entities. Each entity is always represented by its string id, no matter whether the entity is a word, a document or a graph node.\n",
    "\n",
    "**You can download the pre-trained word embedding [HERE](https://thome.isir.upmc.fr/classes/RITAL/word2vec-google-news-300.dat) .**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 19:13:23,544 : INFO : loading KeyedVectors object from word2vec-google-news-300/word2vec-google-news-300.dat\n",
      "2024-02-29 19:13:24,689 : INFO : loading vectors from word2vec-google-news-300/word2vec-google-news-300.dat.vectors.npy with mmap=None\n",
      "2024-02-29 19:13:29,017 : INFO : KeyedVectors lifecycle event {'fname': 'word2vec-google-news-300/word2vec-google-news-300.dat', 'datetime': '2024-02-29T19:13:29.017946', 'gensim': '4.3.2', 'python': '3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "#from gensim.test.utils import get_tmpfile\n",
    "import gensim.downloader as api\n",
    "bload = True\n",
    "#fname = \"word2vec-google-news-300\"\n",
    "fname = \"word2vec-google-news-300\"\n",
    "sdir = \"word2vec-google-news-300/\" # Change\n",
    "\n",
    "if(bload==True):\n",
    "    wv_pre_trained = gensim.models.KeyedVectors.load(sdir+fname+\".dat\")\n",
    "else:    \n",
    "    wv_pre_trained = api.load(fname)\n",
    "    wv_pre_trained.save(sdir+fname+\".dat\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform the \"synctactic\" and \"semantic\" evaluations again. Conclude on the pre-trained embeddings.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4:  sentiment classification\n",
    "\n",
    "In the previous practical session, we used a bag of word approach to transform text into vectors.\n",
    "Here, we propose to try to use word vectors (previously learnt or loaded).\n",
    "\n",
    "\n",
    "### <font color='green'> Since we have only word vectors and that sentences are made of multiple words, we need to aggregate them. </font>\n",
    "\n",
    "\n",
    "### (1) Vectorize reviews using word vectors:\n",
    "\n",
    "Word aggregation can be done in different ways:\n",
    "\n",
    "- Sum\n",
    "- Average\n",
    "- Min/feature\n",
    "- Max/feature\n",
    "\n",
    "#### a few pointers:\n",
    "\n",
    "- `w2v.wv.vocab` is a `set()` of the vocabulary (all existing words in your model)\n",
    "- `np.minimum(a,b) and np.maximum(a,b)` respectively return element-wise min/max "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  -7.57293123   28.71725747  -14.85025887   -7.28830908    8.46466901\n",
      " -108.42054234    1.14369239  176.05374151  -70.98090958  -76.57534255\n",
      "    1.40000819 -106.06666902   -1.98149369   71.09030095    7.09264713\n",
      "  -36.0693451    11.67508647  -62.26880306  -17.52540909 -210.37950162\n",
      "   48.22400982    3.73759637  172.64392945  -97.07995029  -27.64271897\n",
      "   69.14704071  -53.25381788   35.61033983 -102.77474912   83.73235576\n",
      "   72.59939349   27.34860033   31.01915031 -131.62599325  -34.11725628\n",
      "   68.10432259   27.23754314  -89.81583224  -53.09578426 -164.65305753\n",
      "    5.35532321 -148.81550646  -56.12899937   41.6580968   102.65842123\n",
      "  -34.24901678  -71.58212442    5.3329379     1.81644962   13.064842\n",
      "   66.78748237  -92.31527053   48.49156979  -29.12656435  -48.62162613\n",
      "    9.35690386  -18.06858999   34.7728236   -39.10262153   55.61303918\n",
      "   43.64949047  -26.43843784   23.16181149   50.64614199  -91.63264633\n",
      "  129.96082067   -6.19493017   74.68392716  -88.19130325   82.28000903\n",
      "   -8.75215626   91.52637093   77.37580577   14.01366439   95.41985777\n",
      "   15.9366412     8.24134823   18.80178395  -28.0571588    42.338113\n",
      "  -61.80281167  -14.63677272  -86.00896215  126.66467174  -14.61304436\n",
      "    8.33506276   64.16499759   62.0850126   102.95372106    5.83947601\n",
      "  126.80845621   95.35396663   23.39969802  -16.22256728  171.32244274\n",
      "  -10.83398095  127.96574658 -119.32100562   13.63587233  -23.34923922]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection  import train_test_split\n",
    "# We first need to vectorize text:\n",
    "# First we propose to a sum of them\n",
    "\n",
    "def randomvec(val):\n",
    "    default = np.random.randn(val)\n",
    "    default = default  / np.linalg.norm(default)\n",
    "    return default\n",
    "\n",
    "def vectorize(text,mean=False):\n",
    "\n",
    "# \"\"\"\n",
    "# This function should vectorize one review\n",
    "\n",
    "# input: str\n",
    "# output: np.array(float)\n",
    "\n",
    "# \"\"\"      \n",
    "    vec = list()\n",
    "    for word in text:\n",
    "        if not (word in w2v.wv): \n",
    "            vec.append(randomvec(100))\n",
    "        else:\n",
    "            vec.append(w2v.wv[word])\n",
    "            \n",
    "    return sum(vec)\n",
    "\n",
    "lab = [l for t,l in data]  \n",
    "train, test,y_train, y_test = train_test_split(text,lab,test_size=0.2,random_state=42)\n",
    "\n",
    "#classes = [pol for text,pol in train]\n",
    "X = [vectorize(text) for text in train]\n",
    "X_test = [vectorize(text) for text in test]\n",
    "#true = [pol for text,pol in test]\n",
    "\n",
    "#let's see what a review vector looks like.\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.95050251e+01  1.54200372e+01  1.23154661e+01  3.10836614e+01\n",
      " -2.26996226e+01 -2.49738134e+00  7.86992791e+00 -2.29712747e+01\n",
      "  2.74694308e+01  2.77216386e+01 -1.05731094e+01 -3.87175350e+01\n",
      " -2.71817231e+00  1.58782151e+01 -3.57856571e+01  1.48034387e+01\n",
      "  1.29298093e+01  3.40879125e+01 -1.14338176e+00 -1.44684337e+01\n",
      " -2.74474629e+00  1.93755284e+01  6.59190603e+00 -1.90031588e+00\n",
      "  1.94091390e+01 -1.17171934e+01 -2.68422124e+01  2.32434216e+01\n",
      "  1.21691816e+01  5.39238139e+00 -6.91356235e+00 -2.01518819e+00\n",
      " -1.23126532e+01  4.24769607e+00  5.08062781e+00 -3.09706575e+00\n",
      "  6.95105550e+00  4.74856222e-01  1.31316447e+01  2.41026764e+01\n",
      "  3.19635396e+01 -2.15723952e+01  3.40254192e+01 -4.24116698e+00\n",
      " -1.39004307e+00 -2.94744783e+00 -1.31642977e+01  4.80535654e-01\n",
      "  1.99618221e+01  9.01170093e+00 -6.65149044e+00  1.58836373e+01\n",
      " -5.84438930e+00 -1.13945938e+01  2.36811412e+00  7.60873540e+00\n",
      " -3.52722560e+00 -1.99061144e+01  1.08302062e+01 -1.23251594e+01\n",
      " -8.17759768e+00  3.38086724e+01 -2.45959391e+01 -3.43703292e+01\n",
      " -1.09390333e+01 -6.88671020e+00 -7.15944198e+00  2.17931494e+01\n",
      " -1.48837371e+01  2.42950405e+01  1.69523578e+01  1.00500588e+01\n",
      "  2.39719881e+01  4.85020669e+00 -4.06449426e+01 -1.25733051e+01\n",
      "  2.10600322e+01  2.83425749e+01  1.06302865e+01  3.59656784e+01\n",
      "  1.06572102e+01 -2.50222762e+01  1.47267048e+01 -5.50467417e+00\n",
      " -1.52024863e+01 -1.73441947e+01 -2.99023689e+01  3.91966325e+01\n",
      "  3.79241709e+00  5.46259434e+00  7.96194052e+00  1.46227461e+01\n",
      " -1.78014735e+01 -2.79322238e+01 -1.47001549e+01 -1.23676309e+01\n",
      "  2.18874455e+01  1.29192631e+01 -7.41845782e+00  4.91701961e+00\n",
      " -7.63977124e+00 -2.20510423e+01  4.06640799e+00  5.19280447e+00\n",
      " -8.43584378e+00 -1.95261230e+01 -6.75765577e+00 -1.69119366e+01\n",
      "  5.33204078e+00 -2.60449944e+01 -7.62330967e+00 -4.05633077e+00\n",
      "  4.01172733e+00 -3.84011328e+00  1.82184969e+01  8.20615395e+00\n",
      "  1.13207691e+01 -3.45585406e+00  2.83499915e+01  1.76514445e+01\n",
      " -3.85864039e+01 -2.36946143e+00 -2.00136542e+01  6.32714244e+00\n",
      " -1.33683677e+01 -5.58952918e+00 -1.45163720e+01 -1.30025890e+01\n",
      "  3.01281605e-01  9.72808800e+00 -1.93634846e+01 -3.22396893e+01\n",
      " -2.86751054e+01 -5.10815183e+00 -1.25878812e+01 -1.20965020e+01\n",
      "  3.55634643e+00 -8.16569974e+00 -1.30786951e+01  1.97392046e+01\n",
      "  2.27803012e+01 -1.44843197e+01  1.11553971e+01  1.32514381e+00\n",
      "  1.99666810e+01  1.31401938e+01 -1.23770819e+01 -2.40634203e+01\n",
      " -1.74200111e+01  5.67824464e+00  1.62375503e+01  1.25253986e+01\n",
      " -2.30213770e+01  1.88938023e+01 -7.95974338e+00 -7.96862492e+00\n",
      " -1.40718111e+01 -2.63545595e+01 -1.17907281e+01 -9.45416189e+00\n",
      "  9.10037186e-01  1.81900448e+01  9.51532500e+00  1.14460509e+01\n",
      " -2.73949598e+00 -2.55069508e+01  1.84632406e+01 -1.05696789e+01\n",
      "  7.62219407e+00 -2.72379530e+00 -4.27169407e+01 -9.19814332e+00\n",
      " -5.80704197e+00 -2.67148263e+01 -1.49755868e+01 -1.01361227e+01\n",
      "  1.64212637e+01 -2.68238682e+01 -6.34756095e-01 -1.85170931e+00\n",
      " -1.80932335e+01 -1.89344862e+01  3.44426633e+00  9.11408463e+00\n",
      " -4.88486395e+00 -8.03424629e+00 -1.28251523e+01  6.77682877e+00\n",
      "  3.26909907e+01  1.45017896e+01  9.28827008e+00  9.15293622e+00\n",
      "  1.02742782e+01  5.89966683e+00 -2.16703222e+01  7.08611935e+00\n",
      " -1.44032600e+01 -6.34218139e+00 -1.97665794e+01 -2.91987033e+01\n",
      "  1.59526019e+01  7.20481206e+00 -1.41271248e+01  5.56874274e+00\n",
      "  3.71399375e+00 -6.50670955e-01 -1.33925097e+01  2.42637870e+00\n",
      " -1.91210522e+00 -6.43784226e+00 -5.36406656e+00  7.60016817e+00\n",
      " -7.09829296e+00  1.77992463e+01 -3.69115345e+01  8.78414782e+00\n",
      "  2.33065498e+01  4.51836793e-02 -3.23848310e+01 -5.38867755e-02\n",
      " -1.31637606e+00  5.39436872e+00 -1.82525910e+00 -1.30359650e+01\n",
      "  2.61537606e+01 -1.00794928e+01  8.61351288e+00  1.34237754e+01\n",
      "  9.91482189e+00 -6.69022898e+00  1.46962690e+01 -2.41278138e+01\n",
      "  1.04534708e+01  1.24162924e+01  1.88566176e+01 -1.32017319e+00\n",
      "  4.69903618e+00 -8.81165522e+00  2.39137423e+01  8.46402486e+00\n",
      "  1.54835702e+01  3.10213270e+00 -3.14424227e-01 -3.70289995e+01\n",
      " -3.20642536e-01  2.60017627e+00  1.13342092e+01  1.85117328e+01\n",
      " -1.28288554e+00 -1.16643712e+01 -9.20409766e-01  6.40585678e+00\n",
      "  2.04580332e+01  2.52402656e+01  1.33448185e+01 -1.42601184e+01\n",
      "  1.49798933e+01 -9.19183153e-01 -1.88370488e+01 -2.29386590e+01\n",
      " -3.02324845e+00  1.14849676e+00 -2.30041478e+01  1.37788505e+01\n",
      "  1.74031936e+01  5.28534894e+01 -1.20856497e+01 -4.06479965e-01\n",
      " -1.57469831e+01  3.51119215e+00  1.46284681e+01  3.09790948e+01\n",
      "  3.31641404e+01  1.46890445e+01  2.52575435e+01 -1.48124190e+01\n",
      " -1.40738826e+01 -3.19125985e+01 -9.29958754e+00  1.13764458e-01\n",
      "  4.50576794e+00 -8.22203539e+00  1.17519843e+00  2.40982731e+01\n",
      "  7.03222338e+00  1.27890222e+00 -3.55799813e+01 -1.16692935e+01\n",
      "  6.83724755e+00  2.11852097e+01 -2.79311007e+01  3.12347410e+00\n",
      " -3.20632878e+01  1.20501875e+01 -1.01287692e+01 -4.52159433e+00\n",
      "  6.15593393e+00 -1.49246389e+01  1.49907612e+01 -1.81420035e+00]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def vectorize(text,mean_b=False,sum_b = True,max_b=False, min_b=False):     \n",
    "    vec = list()\n",
    "    for word in text:\n",
    "        if not (word in wv_pre_trained): \n",
    "            vec.append(randomvec(300))\n",
    "        else:\n",
    "            vec.append(wv_pre_trained[word])\n",
    "    if mean_b:        \n",
    "        return mean(vec)\n",
    "    if sum_b:        \n",
    "        return sum(vec)\n",
    "    if max_b:        \n",
    "        return max(vec)\n",
    "    else: \n",
    "        return min(vec)\n",
    "\n",
    "lab = [l for t,l in data]  \n",
    "train, test,y_train, y_test = train_test_split(text,lab,test_size=0.2,random_state=42)\n",
    "\n",
    "#classes = [pol for text,pol in train]\n",
    "X = [vectorize(text) for text in train]\n",
    "X_test = [vectorize(text) for text in test]\n",
    "#true = [pol for text,pol in test]\n",
    "\n",
    "#let's see what a review vector looks like.\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Train a classifier \n",
    "as in the previous practical session, train a logistic regression to do sentiment classification with word vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.8346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Scikit Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X,y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"acc:\",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performance should be worst than with bag of word (~80%). Sum/Mean aggregation does not work well on long reviews (especially with many frequent words). This adds a lot of noise.\n",
    "\n",
    "## **Todo** :  Try answering the following questions:\n",
    "\n",
    "- Which word2vec model works best: skip-gram or cbow\n",
    "- Do pretrained vectors work best than those learnt on the train dataset ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 19:23:20,360 : INFO : collecting all words and their counts\n",
      "2024-02-29 19:23:20,362 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-02-29 19:23:20,754 : INFO : PROGRESS: at sentence #10000, processed 2301366 words, keeping 153853 word types\n",
      "2024-02-29 19:23:21,178 : INFO : PROGRESS: at sentence #20000, processed 4553558 words, keeping 240043 word types\n",
      "2024-02-29 19:23:21,396 : INFO : collected 276678 word types from a corpus of 5713167 raw words and 25000 sentences\n",
      "2024-02-29 19:23:21,396 : INFO : Creating a fresh vocabulary\n",
      "2024-02-29 19:23:21,683 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 48208 unique words (17.42% of original 276678, drops 228470)', 'datetime': '2024-02-29T19:23:21.683804', 'gensim': '4.3.2', 'python': '3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2024-02-29 19:23:21,683 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 5389596 word corpus (94.34% of original 5713167, drops 323571)', 'datetime': '2024-02-29T19:23:21.683804', 'gensim': '4.3.2', 'python': '3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2024-02-29 19:23:21,930 : INFO : deleting the raw counts dictionary of 276678 items\n",
      "2024-02-29 19:23:21,945 : INFO : sample=0.001 downsamples 44 most-common words\n",
      "2024-02-29 19:23:21,945 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 4165010.0598832574 word corpus (77.3%% of prior 5389596)', 'datetime': '2024-02-29T19:23:21.945519', 'gensim': '4.3.2', 'python': '3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2024-02-29 19:23:22,301 : INFO : estimated required memory for 48208 words and 100 dimensions: 62670400 bytes\n",
      "2024-02-29 19:23:22,301 : INFO : resetting layer weights\n",
      "2024-02-29 19:23:22,330 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-29T19:23:22.330375', 'gensim': '4.3.2', 'python': '3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'build_vocab'}\n",
      "2024-02-29 19:23:22,331 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 48208 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-02-29T19:23:22.331039', 'gensim': '4.3.2', 'python': '3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
      "2024-02-29 19:23:23,329 : INFO : EPOCH 0 - PROGRESS: at 38.42% examples, 1604814 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:24,335 : INFO : EPOCH 0 - PROGRESS: at 80.64% examples, 1670551 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:24,788 : INFO : EPOCH 0: training on 5713167 raw words (4165202 effective words) took 2.5s, 1696041 effective words/s\n",
      "2024-02-29 19:23:25,794 : INFO : EPOCH 1 - PROGRESS: at 42.38% examples, 1757665 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:26,796 : INFO : EPOCH 1 - PROGRESS: at 85.31% examples, 1767680 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-29 19:23:27,153 : INFO : EPOCH 1: training on 5713167 raw words (4163852 effective words) took 2.4s, 1759173 effective words/s\n",
      "2024-02-29 19:23:28,160 : INFO : EPOCH 2 - PROGRESS: at 42.58% examples, 1768614 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:29,162 : INFO : EPOCH 2 - PROGRESS: at 86.25% examples, 1785499 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:29,495 : INFO : EPOCH 2: training on 5713167 raw words (4164058 effective words) took 2.3s, 1782743 effective words/s\n",
      "2024-02-29 19:23:30,496 : INFO : EPOCH 3 - PROGRESS: at 42.91% examples, 1782076 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:31,502 : INFO : EPOCH 3 - PROGRESS: at 85.69% examples, 1777337 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:31,834 : INFO : EPOCH 3: training on 5713167 raw words (4165775 effective words) took 2.3s, 1781282 effective words/s\n",
      "2024-02-29 19:23:32,841 : INFO : EPOCH 4 - PROGRESS: at 43.58% examples, 1810968 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:33,840 : INFO : EPOCH 4 - PROGRESS: at 85.31% examples, 1766285 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:34,194 : INFO : EPOCH 4: training on 5713167 raw words (4165183 effective words) took 2.4s, 1769392 effective words/s\n",
      "2024-02-29 19:23:34,194 : INFO : Word2Vec lifecycle event {'msg': 'training on 28565835 raw words (20824070 effective words) took 11.9s, 1755328 effective words/s', 'datetime': '2024-02-29T19:23:34.194780', 'gensim': '4.3.2', 'python': '3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
      "2024-02-29 19:23:34,196 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=48208, vector_size=100, alpha=0.025>', 'datetime': '2024-02-29T19:23:34.196047', 'gensim': '4.3.2', 'python': '3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n",
      "2024-02-29 19:23:34,197 : INFO : collecting all words and their counts\n",
      "2024-02-29 19:23:34,197 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-02-29 19:23:34,460 : INFO : PROGRESS: at sentence #10000, processed 2301366 words, keeping 153853 word types\n",
      "2024-02-29 19:23:34,730 : INFO : PROGRESS: at sentence #20000, processed 4553558 words, keeping 240043 word types\n",
      "2024-02-29 19:23:34,877 : INFO : collected 276678 word types from a corpus of 5713167 raw words and 25000 sentences\n",
      "2024-02-29 19:23:34,877 : INFO : Creating a fresh vocabulary\n",
      "2024-02-29 19:23:35,060 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 48208 unique words (17.42% of original 276678, drops 228470)', 'datetime': '2024-02-29T19:23:35.060464', 'gensim': '4.3.2', 'python': '3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2024-02-29 19:23:35,060 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 5389596 word corpus (94.34% of original 5713167, drops 323571)', 'datetime': '2024-02-29T19:23:35.060464', 'gensim': '4.3.2', 'python': '3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2024-02-29 19:23:35,236 : INFO : deleting the raw counts dictionary of 276678 items\n",
      "2024-02-29 19:23:35,240 : INFO : sample=0.001 downsamples 44 most-common words\n",
      "2024-02-29 19:23:35,240 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 4165010.0598832574 word corpus (77.3%% of prior 5389596)', 'datetime': '2024-02-29T19:23:35.240701', 'gensim': '4.3.2', 'python': '3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2024-02-29 19:23:35,510 : INFO : estimated required memory for 48208 words and 100 dimensions: 62670400 bytes\n",
      "2024-02-29 19:23:35,510 : INFO : resetting layer weights\n",
      "2024-02-29 19:23:35,527 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-29T19:23:35.527971', 'gensim': '4.3.2', 'python': '3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'build_vocab'}\n",
      "2024-02-29 19:23:35,527 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 48208 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-02-29T19:23:35.527971', 'gensim': '4.3.2', 'python': '3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
      "2024-02-29 19:23:36,541 : INFO : EPOCH 0 - PROGRESS: at 11.01% examples, 460038 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:37,542 : INFO : EPOCH 0 - PROGRESS: at 22.58% examples, 467265 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:38,554 : INFO : EPOCH 0 - PROGRESS: at 33.74% examples, 464211 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:39,555 : INFO : EPOCH 0 - PROGRESS: at 45.21% examples, 467430 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:40,576 : INFO : EPOCH 0 - PROGRESS: at 57.12% examples, 469994 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:41,578 : INFO : EPOCH 0 - PROGRESS: at 68.56% examples, 471917 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:42,582 : INFO : EPOCH 0 - PROGRESS: at 80.15% examples, 471183 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:43,582 : INFO : EPOCH 0 - PROGRESS: at 91.54% examples, 472353 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:44,297 : INFO : EPOCH 0: training on 5713167 raw words (4164582 effective words) took 8.8s, 475295 effective words/s\n",
      "2024-02-29 19:23:45,319 : INFO : EPOCH 1 - PROGRESS: at 11.92% examples, 491303 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:46,324 : INFO : EPOCH 1 - PROGRESS: at 24.12% examples, 496751 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:47,326 : INFO : EPOCH 1 - PROGRESS: at 36.06% examples, 496761 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:48,332 : INFO : EPOCH 1 - PROGRESS: at 48.04% examples, 495772 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:49,351 : INFO : EPOCH 1 - PROGRESS: at 60.20% examples, 494709 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:50,362 : INFO : EPOCH 1 - PROGRESS: at 71.67% examples, 491700 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:51,360 : INFO : EPOCH 1 - PROGRESS: at 83.69% examples, 491414 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:52,391 : INFO : EPOCH 1 - PROGRESS: at 95.65% examples, 490865 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:52,761 : INFO : EPOCH 1: training on 5713167 raw words (4165693 effective words) took 8.5s, 492347 effective words/s\n",
      "2024-02-29 19:23:53,769 : INFO : EPOCH 2 - PROGRESS: at 11.57% examples, 483369 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-29 19:23:54,775 : INFO : EPOCH 2 - PROGRESS: at 23.80% examples, 492692 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:55,776 : INFO : EPOCH 2 - PROGRESS: at 35.74% examples, 494878 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:56,782 : INFO : EPOCH 2 - PROGRESS: at 47.68% examples, 493824 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:57,797 : INFO : EPOCH 2 - PROGRESS: at 59.56% examples, 490922 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:58,828 : INFO : EPOCH 2 - PROGRESS: at 71.86% examples, 492919 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:23:59,836 : INFO : EPOCH 2 - PROGRESS: at 84.00% examples, 493190 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:24:00,844 : INFO : EPOCH 2 - PROGRESS: at 95.82% examples, 492653 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:24:01,220 : INFO : EPOCH 2: training on 5713167 raw words (4165181 effective words) took 8.5s, 492351 effective words/s\n",
      "2024-02-29 19:24:02,247 : INFO : EPOCH 3 - PROGRESS: at 11.57% examples, 475291 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:24:03,264 : INFO : EPOCH 3 - PROGRESS: at 23.46% examples, 478948 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:24:04,274 : INFO : EPOCH 3 - PROGRESS: at 35.24% examples, 481782 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-29 19:24:05,276 : INFO : EPOCH 3 - PROGRESS: at 47.03% examples, 482757 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:24:06,291 : INFO : EPOCH 3 - PROGRESS: at 58.82% examples, 481626 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:24:07,323 : INFO : EPOCH 3 - PROGRESS: at 70.60% examples, 482051 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-29 19:24:08,340 : INFO : EPOCH 3 - PROGRESS: at 82.83% examples, 483116 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:24:09,344 : INFO : EPOCH 3 - PROGRESS: at 94.21% examples, 482768 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:24:09,825 : INFO : EPOCH 3: training on 5713167 raw words (4165166 effective words) took 8.6s, 484222 effective words/s\n",
      "2024-02-29 19:24:10,832 : INFO : EPOCH 4 - PROGRESS: at 11.40% examples, 476882 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:24:11,842 : INFO : EPOCH 4 - PROGRESS: at 23.46% examples, 485316 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:24:12,854 : INFO : EPOCH 4 - PROGRESS: at 35.40% examples, 488380 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:24:13,852 : INFO : EPOCH 4 - PROGRESS: at 47.34% examples, 489377 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:24:14,856 : INFO : EPOCH 4 - PROGRESS: at 59.38% examples, 490027 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:24:15,862 : INFO : EPOCH 4 - PROGRESS: at 71.12% examples, 490766 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-29 19:24:16,874 : INFO : EPOCH 4 - PROGRESS: at 83.01% examples, 488842 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-29 19:24:17,883 : INFO : EPOCH 4 - PROGRESS: at 94.57% examples, 488462 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-29 19:24:18,342 : INFO : EPOCH 4: training on 5713167 raw words (4164709 effective words) took 8.5s, 489019 effective words/s\n",
      "2024-02-29 19:24:18,342 : INFO : Word2Vec lifecycle event {'msg': 'training on 28565835 raw words (20825331 effective words) took 42.8s, 486409 effective words/s', 'datetime': '2024-02-29T19:24:18.342698', 'gensim': '4.3.2', 'python': '3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
      "2024-02-29 19:24:18,348 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=48208, vector_size=100, alpha=0.025>', 'datetime': '2024-02-29T19:24:18.348384', 'gensim': '4.3.2', 'python': '3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation method: mean\n",
      "Accuracy of CBOW model: 0.7754\n",
      "Accuracy of Skip-gram model: 0.8206\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation method: sum\n",
      "Accuracy of CBOW model: 0.772\n",
      "Accuracy of Skip-gram model: 0.8268\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation method: max\n",
      "Accuracy of CBOW model: 0.676\n",
      "Accuracy of Skip-gram model: 0.5848\n",
      "\n",
      "Aggregation method: min\n",
      "Accuracy of CBOW model: 0.6464\n",
      "Accuracy of Skip-gram model: 0.6954\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train CBOW model\n",
    "cbow_model = gensim.models.Word2Vec(sentences=text,\n",
    "                                    vector_size=100, window=5,\n",
    "                                    min_count=5, sample=0.001,\n",
    "                                    workers=3, sg=0, hs=0, negative=5,\n",
    "                                    cbow_mean=1, epochs=5)\n",
    "\n",
    "# Train Skip-gram model\n",
    "sg_model = gensim.models.Word2Vec(sentences=text,\n",
    "                                   vector_size=100, window=5,\n",
    "                                   min_count=5, sample=0.001,\n",
    "                                   workers=3, sg=1, hs=0, negative=5,\n",
    "                                   cbow_mean=1, epochs=5)\n",
    "\n",
    "# Define vectorize function to accept Word2Vec model directly\n",
    "def vectorize(text, wv_model, aggregation='mean'):\n",
    "    vec = [wv_model[word] if word in wv_model else np.random.rand(100) for word in text]\n",
    "    if aggregation == 'mean':\n",
    "        return np.mean(vec, axis=0)\n",
    "    elif aggregation == 'sum':\n",
    "        return np.sum(vec, axis=0)\n",
    "    elif aggregation == 'max':\n",
    "        return np.max(vec, axis=0)\n",
    "    elif aggregation == 'min':\n",
    "        return np.min(vec, axis=0)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid aggregation method\")\n",
    "\n",
    "# List of aggregation methods\n",
    "aggregation_methods = ['mean', 'sum', 'max', 'min']\n",
    "\n",
    "for method in aggregation_methods:\n",
    "    # Vectorize training and test datasets using CBOW model\n",
    "    X_cbow_train = np.array([vectorize(text, cbow_model.wv, method) for text in train])\n",
    "    X_cbow_test = np.array([vectorize(text, cbow_model.wv, method) for text in test])\n",
    "\n",
    "    # Vectorize training and test datasets using Skip-gram model\n",
    "    X_sg_train = np.array([vectorize(text, sg_model.wv, method) for text in train])\n",
    "    X_sg_test = np.array([vectorize(text, sg_model.wv, method) for text in test])\n",
    "\n",
    "    # Train logistic regression models\n",
    "    lr_cbow = LogisticRegression()\n",
    "    lr_cbow.fit(X_cbow_train, y_train)\n",
    "\n",
    "    lr_sg = LogisticRegression()\n",
    "    lr_sg.fit(X_sg_train, y_train)\n",
    "\n",
    "    # Evaluate accuracy of logistic regression models\n",
    "    y_pred_cbow = lr_cbow.predict(X_cbow_test)\n",
    "    accuracy_cbow = accuracy_score(y_test, y_pred_cbow)\n",
    "\n",
    "    y_pred_sg = lr_sg.predict(X_sg_test)\n",
    "    accuracy_sg = accuracy_score(y_test, y_pred_sg)\n",
    "\n",
    "    print(f\"Aggregation method: {method}\")\n",
    "    print(\"Accuracy of CBOW model:\", accuracy_cbow)\n",
    "    print(\"Accuracy of Skip-gram model:\", accuracy_sg)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "902a52bcf4503a473db011f1937bdfe17613b08622219712e0110e48c4958c23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
